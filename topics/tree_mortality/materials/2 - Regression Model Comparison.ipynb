{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3aaefc1-5c99-4311-ac8c-5f09c3755c56",
   "metadata": {},
   "source": [
    "# Regression Model Comparison\n",
    "\n",
    "In this exercise, you will compare the ability of various regression models to generalize across years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a3b8f-f55a-4f58-b13b-36cf1dd76e48",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, the following cells import required Python modules and define some functions that will be useful for the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9edbd0-8265-4fc5-a224-81db5dc00f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MPLCONFIGDIR=/tmp/.cache\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "%env MPLCONFIGDIR=/tmp/.cache\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneGroupOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a57865a-0ea6-493a-af48-f02a173becbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIRECTORY = '/home/jovyan/shared/NASA_Summer_School_2024/tree_mortality/materials/data'\n",
    "\n",
    "# Data files contain flattened, tabular representation of data for model training\n",
    "DATA_FILE = os.path.join(DATA_DIRECTORY, 'tree_mortality_gridded_training_nonzero.nc4')\n",
    "\n",
    "TRAINING_VARIABLES = [\n",
    "    'PR1-1', 'PRET1-1',\n",
    "    'SPI1-1', 'SPI1-2', 'SPI1-3', 'SPI1-4','SPI1-5', 'SPI1-6',\n",
    "    'SPEI1-1', 'SPEI1-2', 'SPEI1-3', 'SPEI1-4','SPEI1-5', 'SPEI1-6',\n",
    "    'SPI2-1', 'SPI2-2', 'SPI2-3', 'SPI2-4','SPI2-5', 'SPI2-6',\n",
    "    'SPEI2-1', 'SPEI2-2', 'SPEI2-3', 'SPEI2-4','SPEI2-5', 'SPEI2-6',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9478062-ba9b-4f39-9230-9edc766d1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definitions\n",
    "\n",
    "def filter_inf(X, y=None, fill=1e2):\n",
    "    \"\"\"\n",
    "    Returns a version of the dataset in which infinite values have been\n",
    "    replaced with a given finite fill value\n",
    "    \"\"\"\n",
    "    if np.any(np.isnan(X)):\n",
    "        raise ValueError('NaN entries present but not handled')\n",
    "        \n",
    "    Xnew = np.nan_to_num(X, neginf=-fill, posinf=fill)\n",
    "    \n",
    "    return Xnew if y is None else (Xnew, y)\n",
    "\n",
    "\n",
    "def load_training_set(datafile, target='tpa'):\n",
    "    \"\"\"\n",
    "    Loads a training dataset from the specified file, selecting a specific\n",
    "    year if given.\n",
    "\n",
    "    @param datafile: NetCDF4 data file path to load\n",
    "    @param target: the target variable of the regression\n",
    "        (default: tpa, trees per acre)\n",
    "\n",
    "    @return (X, y, years), where X is the feature matrix, y are the\n",
    "        mortality labels, and years are separate years in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Open dataset and select year (if specified)\n",
    "    ds = xr.open_dataset(datafile)\n",
    "\n",
    "    # Drop all variables except for the desired features\n",
    "    X = ds[TRAINING_VARIABLES].to_array().T\n",
    "\n",
    "    # Get target mortality values\n",
    "    y = ds[target].as_numpy()\n",
    "\n",
    "    # Filter infinite values\n",
    "    X, y = filter_inf(X, y)\n",
    "\n",
    "    # Get group definitions (years)\n",
    "    years = ds.year.values.astype(int)\n",
    "    \n",
    "    return X, y, years\n",
    "\n",
    "\n",
    "def get_test_year(years, test_idx):\n",
    "    # Check that there is only a single unique year represented in the test set, and return it\n",
    "    test_year = np.unique(years[test_idx])\n",
    "    assert len(test_year) == 1\n",
    "    return test_year[0]\n",
    "\n",
    "\n",
    "def print_results(results, metric=root_mean_squared_error):\n",
    "    for year, true_pred in sorted(results.items()):\n",
    "        score = metric(*true_pred)\n",
    "        print(f'{year}: {score:.2f}')\n",
    "\n",
    "    all_results = np.vstack([\n",
    "        np.column_stack([true, pred]) for true, pred in results.values()\n",
    "    ])\n",
    "    overall_score = metric(all_results[:, 0], all_results[:, 1])\n",
    "    print(f'Overall Score: {overall_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c979d512-87a1-4b1b-aaee-66bc35d7534b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dacdfb8c2614bf0b4faf1a39e55a7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the training set by year\n",
    "X, y, years = load_training_set(DATA_FILE)\n",
    "\n",
    "# This is a variant of the random forest classifier\n",
    "# You can experiment with alternative models here\n",
    "model = ExtraTreesRegressor(\n",
    "    n_estimators=100, max_depth=4,\n",
    "    bootstrap=True, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Define leave-one-year-out splits\n",
    "cross_val = LeaveOneGroupOut()\n",
    "splits = list(cross_val.split(X, y, years))\n",
    "\n",
    "results_by_year = {}\n",
    "\n",
    "for train_idx, test_idx in tqdm(splits, 'Training'):\n",
    "    test_year = get_test_year(years, test_idx)\n",
    "    model.fit(X[train_idx], y[train_idx])\n",
    "    y_pred = model.predict(X[test_idx])\n",
    "    y_true = y[test_idx]\n",
    "    results_by_year[test_year] = (y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484080d4-6f2d-43ad-8e86-61dc03eb0f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012: 5.01\n",
      "2013: 4.57\n",
      "2014: 6.85\n",
      "2015: 9.88\n",
      "2016: 18.86\n",
      "2017: 15.25\n",
      "2018: 14.78\n",
      "2019: 8.46\n",
      "2021: 10.08\n",
      "Overall Score: 13.50\n"
     ]
    }
   ],
   "source": [
    "print_results(results_by_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df408b-3dbd-4379-9ce8-37422c59797a",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Using the code above as an example, implement and evaluate at least 3 different regression models from [scikit-learn](https://scikit-learn.org/stable/supervised_learning.html), and at least 1 additional \"baseline\" model (e.g., simple linear regression). There are functions above to produce root mean squared error (RMSE) scores for each held-out year.\n",
    "\n",
    "1. Which models exhibit the most success in predicting mortality in the held-out year?\n",
    "2. What factors might explain the difference in model performance across years?\n",
    "3. There is a phenomenon in statistical machine learning called the [\"bias-variance tradeoff\"](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), in which more complex models are better able to fit arbitrary relationships between the input variables and target values (i.e., they exhibit less \"bias\" towards a particular, for example linear, relationship), but this also leads to more \"variance\" in performance due to over-fitting to the particular random sample of data in the training set. Some models have parameters that control the complexity of the relationships that can be learned, such as the `max_depth' parameter of the random forest. Do you observe a bias-variance tradeoff as you explore different models and parameters for this problem? Provide some examples of instances where you observe the effect and how you mitigated it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa784a-0e9c-4dd2-b2f3-413d5ff79b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
